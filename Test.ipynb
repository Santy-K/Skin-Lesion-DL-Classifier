{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4ButEMboKPKU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import time\n",
        "import torchvision.transforms as v2\n",
        "from collections import Counter\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Fix for h5py sometimes not being able to open files in parallel\n",
        "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
        "import h5py\n",
        "\n",
        "class networkTraining():\n",
        "    \"\"\"Class for training and testing a neural network model.\n",
        "    This class handles the training and testing process, including saving the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        \"\"\"Creates the network training class.\n",
        "        This class handles the training and testing process, including saving the model.\n",
        "\n",
        "        Args:\n",
        "            model: Model to be trained.\n",
        "            optimizer: Optimizer to be used.\n",
        "            criterion: Loss function to be used.\n",
        "        \"\"\"\n",
        "        self.device_type = get_device()\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = torch.device(self.device_type)\n",
        "        self.model.to(self.device, non_blocking=True)\n",
        "        self.history = {}\n",
        "\n",
        "    #Adapted fromkuzu_main.py, hw1 of COMP9444\n",
        "    def train(self, train_loader: DataLoader, epoch:int=0, name:str=\"train\"):\n",
        "        \"\"\"Trains the model for one epoch using the given data loader. Epoch is used for logging purposes.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): DataLoader for the training data.\n",
        "            epoch (int, optional): Epoch of the cycle. Used for logging. Defaults to 0.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        correct = 0\n",
        "        loss_total = 0\n",
        "        total_images = 0\n",
        "\n",
        "        t = time.time()\n",
        "        conf_matrix = np.zeros((3, 3))\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
        "            target = target.long()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type=self.device_type):\n",
        "                outputs = self.model(data)\n",
        "                loss = self.criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            loss_total += loss.item()\n",
        "\n",
        "            total_images += len(data)\n",
        "\n",
        "            conf_matrix += confusion_matrix(target.cpu(), pred.cpu())\n",
        "\n",
        "            print(f\"Train Epoch: {epoch} [{total_images}/{len(train_loader.dataset)}]\")\n",
        "\n",
        "        self.history.setdefault(epoch, {})[\"{name}_loss\"] = loss_total / (batch_idx+1)\n",
        "        self.history[epoch][\"{name}_accuracy\"] = correct / len(train_loader.dataset)\n",
        "        self.history[epoch][\"{name}_time\"] = time.time() - t\n",
        "        print(f\"Train Epoch: {epoch}\\t Loss: {loss_total / (batch_idx+1):.6f} \\t Accuracy: {correct}/{len(train_loader.dataset)} ({correct / len(train_loader.dataset):.2%})\")\n",
        "        print(f\"Confusion matrix for train:\")\n",
        "        print(conf_matrix)\n",
        "\n",
        "    #Adapted fromkuzu_main.py, hw1 of COMP9444\n",
        "    def test(self, test_loader: DataLoader, name:str=\"test\", epoch:int=0):\n",
        "        \"\"\"Tests the model using the given data loader. Name is used for logging purposes.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): DataLoader for the training data.\n",
        "            name (str, optional): Name of the test set. Used for logging. Defaults to \"Test\".\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total_samples = 0\n",
        "        batches = len(test_loader)\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "        #inference_mode is faster than no_grad\n",
        "        with torch.inference_mode():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                with autocast(self.device_type):\n",
        "                    outputs = self.model(data)\n",
        "                    target = target.long()\n",
        "                    loss = self.criterion(outputs, target)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                correct += (pred == target).sum().item()\n",
        "                total_samples += target.size(0)\n",
        "\n",
        "        self.history.setdefault(epoch, {})[f\"{name}_loss\"] = test_loss / (batches)\n",
        "        self.history[epoch][f\"{name}_accuracy\"] = correct / len(test_loader.dataset)\n",
        "        self.history[epoch][f\"{name}_time\"] = time.time() - t\n",
        "\n",
        "        print(f\"\\n{name}: Average loss: {test_loss / batches:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({correct / len(test_loader.dataset):.2%}%)\\n\")\n",
        "\n",
        "        #Confusion matrix\n",
        "        conf_matrix = confusion_matrix(target.cpu(), pred.cpu())\n",
        "        print(f\"Confusion matrix for {name}:\")\n",
        "        print(conf_matrix)\n",
        "\n",
        "        return correct / (len(test_loader.dataset))\n",
        "\n",
        "\n",
        "    def save_model(self, path, model_name=\"history/model\"):\n",
        "        #Save the model\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "        #Save the history of the model\n",
        "        df = pd.DataFrame(self.history).T\n",
        "        df = df.rename_axis('epoch').reset_index()\n",
        "        #Create the folder if it does not exist\n",
        "        os.makedirs(os.path.dirname(model_name), exist_ok=True)\n",
        "        df.to_csv(f\"{model_name}_history.csv\", index=False)\n",
        "\n",
        "    def plot_model(self, path='plot.jpg', model_name=\"model\"):\n",
        "        df = pd.DataFrame(self.history).T\n",
        "        df = df.rename_axis(\"epoch\").reset_index()\n",
        "\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "        #Loss and accuracy\n",
        "        loss_list = [x for x in list(df) if \"loss\" in x]\n",
        "        acc_list = [x for x in list(df) if \"accuracy\" in x]\n",
        "\n",
        "        for loss in loss_list:\n",
        "            ax[0].plot(df[\"epoch\"], df[loss], label=loss)\n",
        "\n",
        "        for acc in acc_list:\n",
        "            ax[1].plot(df[\"epoch\"], df[acc], label=acc)\n",
        "\n",
        "        #Labelling and formatting\n",
        "        ax[0].set_title(\"Loss\")\n",
        "        ax[0].set_xlabel(\"Epoch\")\n",
        "        ax[0].set_ylabel(\"Loss\")\n",
        "        ax[0].legend()\n",
        "\n",
        "        ax[1].set_title(\"Accuracy\")\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Accuracy\")\n",
        "        ax[1].yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
        "        ax[1].set_ylim((0, 1))\n",
        "        ax[1].legend()\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(top=0.9)\n",
        "\n",
        "        #Title\n",
        "        fig.suptitle(f\"{model_name} Performance\")\n",
        "        plt.savefig(path)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Finds the device to be used for training and testing.\n",
        "    This is used to determine if a GPU is available.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return 'cuda'\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return 'mps'\n",
        "\n",
        "    return 'cpu'\n",
        "\n",
        "MELANOMA = 1\n",
        "NEITHER = 0\n",
        "SEB = 2\n",
        "def create_h5_from_images(csv_path: str, img_dir: str, h5_filename: str, resolution=(224, 224)):\n",
        "    \"\"\"Creates an HDF5 file from a CSV file containing images and their labels.\n",
        "    Images are stored in uint8 format, where the images are (height, width, channels).\n",
        "    The labels are stored as int.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the CSV file containing image IDs and labels in the form of 'image_id,truth'.\n",
        "        img_dir (str): Relative path to the directory containing the images.\n",
        "        h5_filename (str): Relative path to the output HDF5 file.\n",
        "        resolution (tuple, optional): Height and width of output images. Defaults to (224, 224).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    num_samples = len(df)\n",
        "\n",
        "    with h5py.File(h5_filename, 'w', locking=False) as f:\n",
        "        image_width, image_height = resolution\n",
        "        #Metadata\n",
        "        f.attrs['resolution'] = (image_width, image_height)\n",
        "        f.attrs['num_samples'] = num_samples\n",
        "        f.attrs['num_classes'] = len(df['truth'].unique())\n",
        "\n",
        "\n",
        "        #Dataset of images and labels\n",
        "        dset_images = f.create_dataset(\"images\",\n",
        "                                       shape=(num_samples, image_height, image_width, 3),\n",
        "                                       dtype=np.uint8)\n",
        "\n",
        "        dset_labels = f.create_dataset(\"labels\", shape=(num_samples,), dtype=np.uint8)\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            img_id = row['image_id']\n",
        "            label  = row['truth']  # Labelling\n",
        "            img_path = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
        "\n",
        "            #Resize\n",
        "            with Image.open(img_path).convert('RGB') as img:\n",
        "                transform = v2.Compose([\n",
        "                    v2.Resize(size=(image_height, image_width)),\n",
        "                    #v2.CenterCrop(size=(image_height, image_width)),\n",
        "                ])\n",
        "                img = transform(img)\n",
        "                # Convert to NumPy array\n",
        "                img_np = np.array(img, dtype=np.uint8)\n",
        "\n",
        "            dset_images[i] = img_np\n",
        "            dset_labels[i] = label\n",
        "\n",
        "            if i % 200 == 0:\n",
        "                print(f\"Processed {i}/{len(df['truth'])} images\")\n",
        "\n",
        "    print(f\"Created {h5_filename} with {num_samples} samples.\")\n",
        "\n",
        "class HDF5Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"HDF5 dataset class for loading images and labels from an HDF5 file.\n",
        "    \"\"\"\n",
        "    def __init__(self, filepath: str, transform:str=None):\n",
        "        \"\"\"Create a pytorch dataset from an HDF5 file containing images and labels.\n",
        "        The images are stored in uint8 format, where the images are (height, width, channels).\n",
        "        The labels are stored as int.\n",
        "        The dataset is read-only and the file is closed after reading.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Relative path to the HDF5 file.\n",
        "            transform (str, optional): Transformation of the dataset if required. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.filepath = filepath\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(filepath, 'r') as f:\n",
        "            self.images = f['images'][:]\n",
        "            self.labels = f['labels'][:]\n",
        "            self.length = len(self.labels)\n",
        "            self.all_labels = np.array(f['labels'], dtype=np.uint8)\n",
        "            try:\n",
        "                self.resolution = f.attrs['size']\n",
        "            except KeyError:\n",
        "                self.resolution = (self.images.shape[1], self.images.shape[2])\n",
        "\n",
        "        self.classes = np.unique(self.all_labels)\n",
        "\n",
        "        self.file = None # Initialize file to None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Allows for each worker to open the file independently\n",
        "        if self.file is None:\n",
        "            self.file = h5py.File(self.filepath, 'r')\n",
        "\n",
        "        image = self.file['images'][idx]\n",
        "        label = self.file['labels'][idx]\n",
        "\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def save_transformed_image(tensor, filename, output_dir=\"transformed_images\"):\n",
        "    #Saves a transformed image tensor to a file.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # Add back the mean and std if you want to visualize the original color space\n",
        "    mean = torch.tensor([0.5, 0.5, 0.5]).to(tensor.device)[:, None, None]\n",
        "    std = torch.tensor([0.5, 0.5, 0.5]).to(tensor.device)[:, None, None]\n",
        "    img = tensor * std + mean\n",
        "    img = img.clamp(0, 1)\n",
        "    img = Image.fromarray((img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    img.save(filepath)\n",
        "\n",
        "def add_truth_column(raw: str, modified: str):\n",
        "    \"\"\"Given a csv file with image_id and labels, add a column for truth values.\n",
        "    0 - neither melanoma nor seb\n",
        "    1 - melanoma\n",
        "    2 - seborrheic keratosis\n",
        "\n",
        "    Args:\n",
        "        raw (str): Path to the raw csv file with image_id and labels.\n",
        "        modified (str): Path to the modified csv file with image_id and truth values.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(raw)\n",
        "    output_file = modified\n",
        "    print(df.shape)\n",
        "    df.insert(len(df.columns), \"truth\", NEITHER)\n",
        "    for index, row in df.iterrows():\n",
        "        #intially truth is 0\n",
        "        # assign melanoma as 1\n",
        "        if row['melanoma'] == 1.0 :\n",
        "            df.at[index, 'truth'] = MELANOMA\n",
        "        # assign seb as 2\n",
        "        elif row['seborrheic_keratosis'] == 1.0 :\n",
        "            df.at[index, 'truth'] = SEB\n",
        "\n",
        "    df.to_csv(output_file)\n",
        "\n",
        "def print_label_distribution(h5_file_path):\n",
        "    with h5py.File(h5_file_path, 'r') as f:\n",
        "        labels = f['labels'][:]\n",
        "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "        for label, count in zip(unique_labels, counts):\n",
        "            print(f\"Label {label}: {count} samples\")\n",
        "    return 0\n",
        "\n",
        "def count_samples(dataloader):\n",
        "    labels = np.concatenate([labels.numpy() for _, labels in dataloader])\n",
        "    return Counter(labels)\n",
        "\n",
        "def seed_program(seed=1):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def convertdata():\n",
        "    #Example usage of dataset creation\n",
        "    add_truth_column(\"data/train.csv\", \"data/train_truth_1.csv\")\n",
        "    add_truth_column(\"data/validation.csv\", \"data/validation_truth.csv\")\n",
        "    add_truth_column(\"data/test.csv\", \"data/test_truth_1.csv\")\n",
        "    create_h5_from_images(\"data/train_truth_1.csv\", \"data/train\", \"data/train.h5\", (128, 128))\n",
        "    create_h5_from_images(\"data/validation_truth.csv\", \"data/validation\", \"data/valid.h5\", (128, 128))\n",
        "    create_h5_from_images(\"data/test_truth_1.csv\", \"data/test\", \"data/test.h5\", (128, 128))\n",
        "\n",
        "    #We can see the distribution of the labels in the dataset\n",
        "    print_label_distribution(\"data/train.h5\")\n",
        "    print_label_distribution(\"data/valid.h5\")\n",
        "    print_label_distribution(\"data/test.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XG3_Wyy7-yd-"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "def MobileNetV3Model(num_classes=3):\n",
        "    model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
        "    #Freeze the layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Change the last layer to match the number of classes\n",
        "    in_features = 960\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(in_features, 120),       \n",
        "        nn.BatchNorm1d(120),               \n",
        "        nn.ReLU(),                         \n",
        "        nn.Linear(120, 85),\n",
        "        nn.BatchNorm1d(85),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.2),                 \n",
        "        nn.Linear(85, num_classes)         \n",
        "    )\n",
        "    return model\n",
        "\n",
        "def AlexNetModel(num_classes=3):\n",
        "    model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
        "    #Freeze the layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Change the last layer to match the number of classes\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(in_features, 120),       \n",
        "        nn.BatchNorm1d(120),               \n",
        "        nn.ReLU(),                         \n",
        "        nn.Linear(120, 85),\n",
        "        nn.BatchNorm1d(85),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.2),                 \n",
        "        nn.Linear(85, num_classes)    \n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iycKVOcgFhD_",
        "outputId": "c70392d6-dbaa-468f-be04-d86d5501861d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforms\n",
            "Data size:  2750\n",
            "Class counts:  [1843  386  521]\n",
            "Data loading\n",
            "Train size:  2337 Counter({0: 1566, 2: 443, 1: 328})\n",
            "Valid size:  413 Counter({0: 277, 2: 78, 1: 58})\n",
            "Training epoch:  0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 115\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting epoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m    118\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(accuracy, trainer\u001b[38;5;241m.\u001b[39mtest(valid_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch))\n",
            "Cell \u001b[1;32mIn[10], line 62\u001b[0m, in \u001b[0;36mnetworkTraining.train\u001b[1;34m(self, train_loader, epoch, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_type):\n\u001b[1;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, target)\n\u001b[0;32m     64\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "#Set the seed for reproducibility\n",
        "seed_program(seed=1)\n",
        "\n",
        "#0.1 Data preprocessing variables\n",
        "image_scale = (224, 224)\n",
        "\n",
        "mean_vars = [0.485, 0.456, 0.406]\n",
        "std_vars = [0.229, 0.224, 0.225]\n",
        "\n",
        "transforms_2017 = v2.Compose([\n",
        "    v2.Resize(image_scale),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=mean_vars, std=std_vars),\n",
        "    v2.ToTensor()\n",
        "])\n",
        "\n",
        "transforms_train = v2.Compose([\n",
        "    v2.RandomResizedCrop(scale=(0.8, 1.0), size=image_scale,ratio=(0.9, 1.1)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.5),\n",
        "    v2.RandomRotation(degrees=45),\n",
        "\n",
        "    v2.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=mean_vars, std=std_vars),\n",
        "    v2.ToTensor()\n",
        "])\n",
        "\n",
        "print(\"Transforms\")\n",
        "batch_size = 256\n",
        "\n",
        "#Read Data\n",
        "data_name = \"data/all_224.h5\"\n",
        "data = HDF5Dataset(data_name)\n",
        "print(\"Data size: \", len(data))\n",
        "#Print class breakdown\n",
        "class_counts = np.bincount(data.all_labels)\n",
        "print(\"Class counts: \", class_counts)\n",
        "\n",
        "#Train, test, valid split via stratified sampling\n",
        "#2000, 550, 200\n",
        "train_idx, valid_idx, train_label, _ = train_test_split(\n",
        "    np.arange(len(data.all_labels)), data.all_labels, stratify=data.all_labels, test_size=0.15, random_state=42\n",
        ")\n",
        "print(\"Data loading\")\n",
        "train_dataset = HDF5Dataset(data_name, transform=transforms_train)\n",
        "valid_dataset = HDF5Dataset(data_name, transform=transforms_2017)\n",
        "\n",
        "train_data = torch.utils.data.Subset(train_dataset, train_idx)\n",
        "valid_data = torch.utils.data.Subset(valid_dataset, valid_idx)\n",
        "\n",
        "class_weights = 1.0 / class_counts\n",
        "weights = class_weights[train_data.dataset.all_labels[train_data.indices]]\n",
        "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
        "\n",
        "#Sizes of the datasets\n",
        "print(\"Train size: \", len(train_data), Counter(train_data.dataset.all_labels[train_data.indices]))\n",
        "print(\"Valid size: \", len(valid_data), Counter(valid_data.dataset.all_labels[valid_data.indices]))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler,\n",
        "    shuffle = False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_data,\n",
        "    batch_size=len(valid_data),\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "#Testing the data loading. This is SLOW, so only run to verify the data loading is correct.\n",
        "if 0 == 1:\n",
        "    print(\"After loading data:\")\n",
        "    print(f\"Train: {count_samples(train_loader)}\")\n",
        "    print(f\"Valid: {count_samples(valid_loader)}\")\n",
        "\n",
        "    exit()\n",
        "\n",
        "#2. Hyperparameters, standard for reference\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 75\n",
        "weight_decay = 1e-4\n",
        "\n",
        "#3. Model\n",
        "#model = AlexNetModel(num_classes=3)\n",
        "model = MobileNetV3Model(num_classes=3)\n",
        "#4. Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "name = \"mobilenet\"\n",
        "accuracy = 0\n",
        "best_accuracy = 0\n",
        "best_model = None\n",
        "\n",
        "#5. Training and testing\n",
        "trainer = networkTraining(model, optimizer, criterion)\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Training epoch: \", epoch)\n",
        "    trainer.train(train_loader, epoch)\n",
        "\n",
        "    print(\"Testing epoch: \", epoch)\n",
        "    accuracy = max(accuracy, trainer.test(valid_loader, \"Validation\", epoch))\n",
        "    \n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model_file = f\"models/{name}_{epoch}.pth\"\n",
        "        best_epoch = epoch\n",
        "        print(f\"New best accuracy: {best_accuracy}\")\n",
        "        #Save the model\n",
        "        trainer.save_model(f\"models/{name}_{epoch}.pth\", model_name=f\"history/{name}\")\n",
        "\n",
        "#Save the model after final testing\n",
        "trainer.save_model(f\"models/{name}_{num_epochs}.pth\", model_name=f\"history/{name}\")\n",
        "\n",
        "trainer.plot_model(model_name=\"MobileNet\", path=f\"plots/{name}.jpg\")\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5yiYdz3qxDq",
        "outputId": "49bd36fc-d3cf-45e3-bedb-6776a649feb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test: Average loss: 0.6225, Accuracy: 149/200 (74.50%%)\n",
            "\n",
            "Confusion matrix for Test:\n",
            "[[111   5  18]\n",
            " [  6  18   4]\n",
            " [ 11   7  20]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = trainer.device\n",
        "BEST_MODEL_PATH = best_model_file\n",
        "\n",
        "state_dict = torch.load(BEST_MODEL_PATH, map_location=device)\n",
        "\n",
        "trainer.model.load_state_dict(state_dict)\n",
        "\n",
        "trainer.test(valid_loader, \"Valid\", 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLogXdk050Iv",
        "outputId": "2f8096d1-2c73-4006-e2a8-aa8bfa0aa162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Switched to new optimizer with LR: 0.0001\n",
            "\n",
            "Validation: Average loss: 0.6225, Accuracy: 149/200 (74.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[111   5  18]\n",
            " [  6  18   4]\n",
            " [ 11   7  20]]\n",
            "Fine-tuning epoch: 26\n",
            "Train Epoch: 26 [256/2000]\n",
            "Train Epoch: 26 [512/2000]\n",
            "Train Epoch: 26 [768/2000]\n",
            "Train Epoch: 26 [1024/2000]\n",
            "Train Epoch: 26 [1280/2000]\n",
            "Train Epoch: 26 [1536/2000]\n",
            "Train Epoch: 26 [1792/2000]\n",
            "Train Epoch: 26 [2000/2000]\n",
            "Train Epoch: 26\t Loss: 0.655471 \t Accuracy: 1517/2000 (75.85%)\n",
            "Confusion matrix for train:\n",
            "[[473.  69. 140.]\n",
            " [ 48. 560.  47.]\n",
            " [120.  59. 484.]]\n",
            "Validating fine-tuning epoch: 26\n",
            "\n",
            "Validation: Average loss: 0.6148, Accuracy: 148/200 (74.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[108   8  18]\n",
            " [  4  20   4]\n",
            " [ 12   6  20]]\n",
            "Fine-tuning epoch: 27\n",
            "Train Epoch: 27 [256/2000]\n",
            "Train Epoch: 27 [512/2000]\n",
            "Train Epoch: 27 [768/2000]\n",
            "Train Epoch: 27 [1024/2000]\n",
            "Train Epoch: 27 [1280/2000]\n",
            "Train Epoch: 27 [1536/2000]\n",
            "Train Epoch: 27 [1792/2000]\n",
            "Train Epoch: 27 [2000/2000]\n",
            "Train Epoch: 27\t Loss: 0.611008 \t Accuracy: 1575/2000 (78.75%)\n",
            "Confusion matrix for train:\n",
            "[[475.  53. 102.]\n",
            " [ 42. 586.  57.]\n",
            " [126.  45. 514.]]\n",
            "Validating fine-tuning epoch: 27\n",
            "\n",
            "Validation: Average loss: 0.6026, Accuracy: 146/200 (73.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[108   5  21]\n",
            " [  3  18   7]\n",
            " [ 14   4  20]]\n",
            "Fine-tuning epoch: 28\n",
            "Train Epoch: 28 [256/2000]\n",
            "Train Epoch: 28 [512/2000]\n",
            "Train Epoch: 28 [768/2000]\n",
            "Train Epoch: 28 [1024/2000]\n",
            "Train Epoch: 28 [1280/2000]\n",
            "Train Epoch: 28 [1536/2000]\n",
            "Train Epoch: 28 [1792/2000]\n",
            "Train Epoch: 28 [2000/2000]\n",
            "Train Epoch: 28\t Loss: 0.559772 \t Accuracy: 1595/2000 (79.75%)\n",
            "Confusion matrix for train:\n",
            "[[494.  54. 123.]\n",
            " [ 30. 574.  47.]\n",
            " [112.  39. 527.]]\n",
            "Validating fine-tuning epoch: 28\n",
            "\n",
            "Validation: Average loss: 0.5922, Accuracy: 144/200 (72.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[104   5  25]\n",
            " [  1  20   7]\n",
            " [ 14   4  20]]\n",
            "Fine-tuning epoch: 29\n",
            "Train Epoch: 29 [256/2000]\n",
            "Train Epoch: 29 [512/2000]\n",
            "Train Epoch: 29 [768/2000]\n",
            "Train Epoch: 29 [1024/2000]\n",
            "Train Epoch: 29 [1280/2000]\n",
            "Train Epoch: 29 [1536/2000]\n",
            "Train Epoch: 29 [1792/2000]\n",
            "Train Epoch: 29 [2000/2000]\n",
            "Train Epoch: 29\t Loss: 0.568333 \t Accuracy: 1596/2000 (79.80%)\n",
            "Confusion matrix for train:\n",
            "[[509.  70. 114.]\n",
            " [ 33. 575.  39.]\n",
            " [100.  48. 512.]]\n",
            "Validating fine-tuning epoch: 29\n",
            "\n",
            "Validation: Average loss: 0.5874, Accuracy: 146/200 (73.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[104   5  25]\n",
            " [  1  21   6]\n",
            " [ 14   3  21]]\n",
            "Fine-tuning epoch: 30\n",
            "Train Epoch: 30 [256/2000]\n",
            "Train Epoch: 30 [512/2000]\n",
            "Train Epoch: 30 [768/2000]\n",
            "Train Epoch: 30 [1024/2000]\n",
            "Train Epoch: 30 [1280/2000]\n",
            "Train Epoch: 30 [1536/2000]\n",
            "Train Epoch: 30 [1792/2000]\n",
            "Train Epoch: 30 [2000/2000]\n",
            "Train Epoch: 30\t Loss: 0.559349 \t Accuracy: 1606/2000 (80.30%)\n",
            "Confusion matrix for train:\n",
            "[[483.  63.  95.]\n",
            " [ 36. 575.  30.]\n",
            " [131.  39. 548.]]\n",
            "Validating fine-tuning epoch: 30\n",
            "\n",
            "Validation: Average loss: 0.5731, Accuracy: 151/200 (75.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[107   3  24]\n",
            " [  1  21   6]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 31\n",
            "Train Epoch: 31 [256/2000]\n",
            "Train Epoch: 31 [512/2000]\n",
            "Train Epoch: 31 [768/2000]\n",
            "Train Epoch: 31 [1024/2000]\n",
            "Train Epoch: 31 [1280/2000]\n",
            "Train Epoch: 31 [1536/2000]\n",
            "Train Epoch: 31 [1792/2000]\n",
            "Train Epoch: 31 [2000/2000]\n",
            "Train Epoch: 31\t Loss: 0.519235 \t Accuracy: 1651/2000 (82.55%)\n",
            "Confusion matrix for train:\n",
            "[[517.  59. 100.]\n",
            " [ 29. 559.  29.]\n",
            " [105.  27. 575.]]\n",
            "Validating fine-tuning epoch: 31\n",
            "\n",
            "Validation: Average loss: 0.5765, Accuracy: 147/200 (73.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[108   3  23]\n",
            " [  4  16   8]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 32\n",
            "Train Epoch: 32 [256/2000]\n",
            "Train Epoch: 32 [512/2000]\n",
            "Train Epoch: 32 [768/2000]\n",
            "Train Epoch: 32 [1024/2000]\n",
            "Train Epoch: 32 [1280/2000]\n",
            "Train Epoch: 32 [1536/2000]\n",
            "Train Epoch: 32 [1792/2000]\n",
            "Train Epoch: 32 [2000/2000]\n",
            "Train Epoch: 32\t Loss: 0.480759 \t Accuracy: 1684/2000 (84.20%)\n",
            "Confusion matrix for train:\n",
            "[[494.  38.  87.]\n",
            " [ 27. 625.  38.]\n",
            " [111.  15. 565.]]\n",
            "Validating fine-tuning epoch: 32\n",
            "\n",
            "Validation: Average loss: 0.5664, Accuracy: 152/200 (76.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[111   4  19]\n",
            " [  4  19   5]\n",
            " [ 14   2  22]]\n",
            "Fine-tuning epoch: 33\n",
            "Train Epoch: 33 [256/2000]\n",
            "Train Epoch: 33 [512/2000]\n",
            "Train Epoch: 33 [768/2000]\n",
            "Train Epoch: 33 [1024/2000]\n",
            "Train Epoch: 33 [1280/2000]\n",
            "Train Epoch: 33 [1536/2000]\n",
            "Train Epoch: 33 [1792/2000]\n",
            "Train Epoch: 33 [2000/2000]\n",
            "Train Epoch: 33\t Loss: 0.471579 \t Accuracy: 1693/2000 (84.65%)\n",
            "Confusion matrix for train:\n",
            "[[511.  42. 106.]\n",
            " [ 23. 631.  30.]\n",
            " [ 79.  27. 551.]]\n",
            "Validating fine-tuning epoch: 33\n",
            "\n",
            "Validation: Average loss: 0.5747, Accuracy: 152/200 (76.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[107   5  22]\n",
            " [  1  21   6]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 34\n",
            "Train Epoch: 34 [256/2000]\n",
            "Train Epoch: 34 [512/2000]\n",
            "Train Epoch: 34 [768/2000]\n",
            "Train Epoch: 34 [1024/2000]\n",
            "Train Epoch: 34 [1280/2000]\n",
            "Train Epoch: 34 [1536/2000]\n",
            "Train Epoch: 34 [1792/2000]\n",
            "Train Epoch: 34 [2000/2000]\n",
            "Train Epoch: 34\t Loss: 0.444265 \t Accuracy: 1704/2000 (85.20%)\n",
            "Confusion matrix for train:\n",
            "[[528.  47. 101.]\n",
            " [ 18. 644.  21.]\n",
            " [ 79.  30. 532.]]\n",
            "Validating fine-tuning epoch: 34\n",
            "\n",
            "Validation: Average loss: 0.5642, Accuracy: 155/200 (77.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[110   3  21]\n",
            " [  1  21   6]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 35\n",
            "Train Epoch: 35 [256/2000]\n",
            "Train Epoch: 35 [512/2000]\n",
            "Train Epoch: 35 [768/2000]\n",
            "Train Epoch: 35 [1024/2000]\n",
            "Train Epoch: 35 [1280/2000]\n",
            "Train Epoch: 35 [1536/2000]\n",
            "Train Epoch: 35 [1792/2000]\n",
            "Train Epoch: 35 [2000/2000]\n",
            "Train Epoch: 35\t Loss: 0.410155 \t Accuracy: 1739/2000 (86.95%)\n",
            "Confusion matrix for train:\n",
            "[[522.  34.  90.]\n",
            " [ 21. 651.  29.]\n",
            " [ 78.   9. 566.]]\n",
            "Validating fine-tuning epoch: 35\n",
            "\n",
            "Validation: Average loss: 0.5137, Accuracy: 160/200 (80.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[117   4  13]\n",
            " [  1  21   6]\n",
            " [ 14   2  22]]\n",
            "Fine-tuning epoch: 36\n",
            "Train Epoch: 36 [256/2000]\n",
            "Train Epoch: 36 [512/2000]\n",
            "Train Epoch: 36 [768/2000]\n",
            "Train Epoch: 36 [1024/2000]\n",
            "Train Epoch: 36 [1280/2000]\n",
            "Train Epoch: 36 [1536/2000]\n",
            "Train Epoch: 36 [1792/2000]\n",
            "Train Epoch: 36 [2000/2000]\n",
            "Train Epoch: 36\t Loss: 0.386626 \t Accuracy: 1768/2000 (88.40%)\n",
            "Confusion matrix for train:\n",
            "[[567.  24.  82.]\n",
            " [  9. 647.  25.]\n",
            " [ 79.  13. 554.]]\n",
            "Validating fine-tuning epoch: 36\n",
            "\n",
            "Validation: Average loss: 0.5064, Accuracy: 161/200 (80.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[117   4  13]\n",
            " [  2  21   5]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 37\n",
            "Train Epoch: 37 [256/2000]\n",
            "Train Epoch: 37 [512/2000]\n",
            "Train Epoch: 37 [768/2000]\n",
            "Train Epoch: 37 [1024/2000]\n",
            "Train Epoch: 37 [1280/2000]\n",
            "Train Epoch: 37 [1536/2000]\n",
            "Train Epoch: 37 [1792/2000]\n",
            "Train Epoch: 37 [2000/2000]\n",
            "Train Epoch: 37\t Loss: 0.414355 \t Accuracy: 1733/2000 (86.65%)\n",
            "Confusion matrix for train:\n",
            "[[526.  35.  84.]\n",
            " [ 18. 647.  22.]\n",
            " [ 91.  17. 560.]]\n",
            "Validating fine-tuning epoch: 37\n",
            "\n",
            "Validation: Average loss: 0.5320, Accuracy: 159/200 (79.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[115   4  15]\n",
            " [  3  21   4]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 38\n",
            "Train Epoch: 38 [256/2000]\n",
            "Train Epoch: 38 [512/2000]\n",
            "Train Epoch: 38 [768/2000]\n",
            "Train Epoch: 38 [1024/2000]\n",
            "Train Epoch: 38 [1280/2000]\n",
            "Train Epoch: 38 [1536/2000]\n",
            "Train Epoch: 38 [1792/2000]\n",
            "Train Epoch: 38 [2000/2000]\n",
            "Train Epoch: 38\t Loss: 0.384857 \t Accuracy: 1757/2000 (87.85%)\n",
            "Confusion matrix for train:\n",
            "[[560.  36.  89.]\n",
            " [  7. 633.  14.]\n",
            " [ 83.  14. 564.]]\n",
            "Validating fine-tuning epoch: 38\n",
            "\n",
            "Validation: Average loss: 0.5849, Accuracy: 150/200 (75.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[105   4  25]\n",
            " [  1  22   5]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 39\n",
            "Train Epoch: 39 [256/2000]\n",
            "Train Epoch: 39 [512/2000]\n",
            "Train Epoch: 39 [768/2000]\n",
            "Train Epoch: 39 [1024/2000]\n",
            "Train Epoch: 39 [1280/2000]\n",
            "Train Epoch: 39 [1536/2000]\n",
            "Train Epoch: 39 [1792/2000]\n",
            "Train Epoch: 39 [2000/2000]\n",
            "Train Epoch: 39\t Loss: 0.376686 \t Accuracy: 1761/2000 (88.05%)\n",
            "Confusion matrix for train:\n",
            "[[545.  37. 108.]\n",
            " [  9. 661.  13.]\n",
            " [ 71.   1. 555.]]\n",
            "Validating fine-tuning epoch: 39\n",
            "\n",
            "Validation: Average loss: 0.6100, Accuracy: 148/200 (74.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[102   4  28]\n",
            " [  1  22   5]\n",
            " [ 11   3  24]]\n",
            "Fine-tuning epoch: 40\n",
            "Train Epoch: 40 [256/2000]\n",
            "Train Epoch: 40 [512/2000]\n",
            "Train Epoch: 40 [768/2000]\n",
            "Train Epoch: 40 [1024/2000]\n",
            "Train Epoch: 40 [1280/2000]\n",
            "Train Epoch: 40 [1536/2000]\n",
            "Train Epoch: 40 [1792/2000]\n",
            "Train Epoch: 40 [2000/2000]\n",
            "Train Epoch: 40\t Loss: 0.350330 \t Accuracy: 1798/2000 (89.90%)\n",
            "Confusion matrix for train:\n",
            "[[565.  37.  63.]\n",
            " [  8. 624.   9.]\n",
            " [ 67.  18. 609.]]\n",
            "Validating fine-tuning epoch: 40\n",
            "\n",
            "Validation: Average loss: 0.5451, Accuracy: 157/200 (78.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[112   2  20]\n",
            " [  1  21   6]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 41\n",
            "Train Epoch: 41 [256/2000]\n",
            "Train Epoch: 41 [512/2000]\n",
            "Train Epoch: 41 [768/2000]\n",
            "Train Epoch: 41 [1024/2000]\n",
            "Train Epoch: 41 [1280/2000]\n",
            "Train Epoch: 41 [1536/2000]\n",
            "Train Epoch: 41 [1792/2000]\n",
            "Train Epoch: 41 [2000/2000]\n",
            "Train Epoch: 41\t Loss: 0.328738 \t Accuracy: 1822/2000 (91.10%)\n",
            "Confusion matrix for train:\n",
            "[[578.  30.  68.]\n",
            " [  6. 629.   4.]\n",
            " [ 61.   9. 615.]]\n",
            "Validating fine-tuning epoch: 41\n",
            "\n",
            "Validation: Average loss: 0.5362, Accuracy: 159/200 (79.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[117   2  15]\n",
            " [  6  18   4]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 42\n",
            "Train Epoch: 42 [256/2000]\n",
            "Train Epoch: 42 [512/2000]\n",
            "Train Epoch: 42 [768/2000]\n",
            "Train Epoch: 42 [1024/2000]\n",
            "Train Epoch: 42 [1280/2000]\n",
            "Train Epoch: 42 [1536/2000]\n",
            "Train Epoch: 42 [1792/2000]\n",
            "Train Epoch: 42 [2000/2000]\n",
            "Train Epoch: 42\t Loss: 0.311117 \t Accuracy: 1824/2000 (91.20%)\n",
            "Confusion matrix for train:\n",
            "[[578.  23.  69.]\n",
            " [  8. 650.   6.]\n",
            " [ 61.   9. 596.]]\n",
            "Validating fine-tuning epoch: 42\n",
            "\n",
            "Validation: Average loss: 0.5351, Accuracy: 156/200 (78.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[115   2  17]\n",
            " [  7  17   4]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 43\n",
            "Train Epoch: 43 [256/2000]\n",
            "Train Epoch: 43 [512/2000]\n",
            "Train Epoch: 43 [768/2000]\n",
            "Train Epoch: 43 [1024/2000]\n",
            "Train Epoch: 43 [1280/2000]\n",
            "Train Epoch: 43 [1536/2000]\n",
            "Train Epoch: 43 [1792/2000]\n",
            "Train Epoch: 43 [2000/2000]\n",
            "Train Epoch: 43\t Loss: 0.290400 \t Accuracy: 1842/2000 (92.10%)\n",
            "Confusion matrix for train:\n",
            "[[616.  17.  72.]\n",
            " [ 10. 627.   8.]\n",
            " [ 47.   4. 599.]]\n",
            "Validating fine-tuning epoch: 43\n",
            "\n",
            "Validation: Average loss: 0.5289, Accuracy: 155/200 (77.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[114   1  19]\n",
            " [  6  18   4]\n",
            " [ 13   2  23]]\n",
            "Fine-tuning epoch: 44\n",
            "Train Epoch: 44 [256/2000]\n",
            "Train Epoch: 44 [512/2000]\n",
            "Train Epoch: 44 [768/2000]\n",
            "Train Epoch: 44 [1024/2000]\n",
            "Train Epoch: 44 [1280/2000]\n",
            "Train Epoch: 44 [1536/2000]\n",
            "Train Epoch: 44 [1792/2000]\n",
            "Train Epoch: 44 [2000/2000]\n",
            "Train Epoch: 44\t Loss: 0.287401 \t Accuracy: 1832/2000 (91.60%)\n",
            "Confusion matrix for train:\n",
            "[[544.  20.  58.]\n",
            " [  6. 680.   9.]\n",
            " [ 73.   2. 608.]]\n",
            "Validating fine-tuning epoch: 44\n",
            "\n",
            "Validation: Average loss: 0.5463, Accuracy: 155/200 (77.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[110   0  24]\n",
            " [  3  23   2]\n",
            " [ 13   3  22]]\n",
            "Fine-tuning epoch: 45\n",
            "Train Epoch: 45 [256/2000]\n",
            "Train Epoch: 45 [512/2000]\n",
            "Train Epoch: 45 [768/2000]\n",
            "Train Epoch: 45 [1024/2000]\n",
            "Train Epoch: 45 [1280/2000]\n",
            "Train Epoch: 45 [1536/2000]\n",
            "Train Epoch: 45 [1792/2000]\n",
            "Train Epoch: 45 [2000/2000]\n",
            "Train Epoch: 45\t Loss: 0.284257 \t Accuracy: 1847/2000 (92.35%)\n",
            "Confusion matrix for train:\n",
            "[[585.  22.  57.]\n",
            " [  7. 622.   7.]\n",
            " [ 50.  10. 640.]]\n",
            "Validating fine-tuning epoch: 45\n",
            "\n",
            "Validation: Average loss: 0.5474, Accuracy: 156/200 (78.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[109   0  25]\n",
            " [  1  24   3]\n",
            " [ 12   3  23]]\n",
            "Fine-tuning epoch: 46\n",
            "Train Epoch: 46 [256/2000]\n",
            "Train Epoch: 46 [512/2000]\n",
            "Train Epoch: 46 [768/2000]\n",
            "Train Epoch: 46 [1024/2000]\n",
            "Train Epoch: 46 [1280/2000]\n",
            "Train Epoch: 46 [1536/2000]\n",
            "Train Epoch: 46 [1792/2000]\n",
            "Train Epoch: 46 [2000/2000]\n",
            "Train Epoch: 46\t Loss: 0.265406 \t Accuracy: 1859/2000 (92.95%)\n",
            "Confusion matrix for train:\n",
            "[[594.  17.  68.]\n",
            " [  3. 663.   5.]\n",
            " [ 46.   2. 602.]]\n",
            "Validating fine-tuning epoch: 46\n",
            "\n",
            "Validation: Average loss: 0.5535, Accuracy: 157/200 (78.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[112   0  22]\n",
            " [  3  21   4]\n",
            " [ 11   3  24]]\n",
            "Fine-tuning epoch: 47\n",
            "Train Epoch: 47 [256/2000]\n",
            "Train Epoch: 47 [512/2000]\n",
            "Train Epoch: 47 [768/2000]\n",
            "Train Epoch: 47 [1024/2000]\n",
            "Train Epoch: 47 [1280/2000]\n",
            "Train Epoch: 47 [1536/2000]\n",
            "Train Epoch: 47 [1792/2000]\n",
            "Train Epoch: 47 [2000/2000]\n",
            "Train Epoch: 47\t Loss: 0.256796 \t Accuracy: 1869/2000 (93.45%)\n",
            "Confusion matrix for train:\n",
            "[[606.  19.  67.]\n",
            " [  4. 666.   3.]\n",
            " [ 34.   4. 597.]]\n",
            "Validating fine-tuning epoch: 47\n",
            "\n",
            "Validation: Average loss: 0.5337, Accuracy: 160/200 (80.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[117   1  16]\n",
            " [  6  21   1]\n",
            " [ 13   3  22]]\n",
            "Fine-tuning epoch: 48\n",
            "Train Epoch: 48 [256/2000]\n",
            "Train Epoch: 48 [512/2000]\n",
            "Train Epoch: 48 [768/2000]\n",
            "Train Epoch: 48 [1024/2000]\n",
            "Train Epoch: 48 [1280/2000]\n",
            "Train Epoch: 48 [1536/2000]\n",
            "Train Epoch: 48 [1792/2000]\n",
            "Train Epoch: 48 [2000/2000]\n",
            "Train Epoch: 48\t Loss: 0.245553 \t Accuracy: 1866/2000 (93.30%)\n",
            "Confusion matrix for train:\n",
            "[[608.  18.  55.]\n",
            " [  4. 627.   2.]\n",
            " [ 49.   6. 631.]]\n",
            "Validating fine-tuning epoch: 48\n",
            "\n",
            "Validation: Average loss: 0.5305, Accuracy: 161/200 (80.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[120   1  13]\n",
            " [  6  20   2]\n",
            " [ 14   3  21]]\n",
            "Fine-tuning epoch: 49\n",
            "Train Epoch: 49 [256/2000]\n",
            "Train Epoch: 49 [512/2000]\n",
            "Train Epoch: 49 [768/2000]\n",
            "Train Epoch: 49 [1024/2000]\n",
            "Train Epoch: 49 [1280/2000]\n",
            "Train Epoch: 49 [1536/2000]\n",
            "Train Epoch: 49 [1792/2000]\n",
            "Train Epoch: 49 [2000/2000]\n",
            "Train Epoch: 49\t Loss: 0.224908 \t Accuracy: 1883/2000 (94.15%)\n",
            "Confusion matrix for train:\n",
            "[[588.  12.  57.]\n",
            " [  4. 677.   4.]\n",
            " [ 38.   2. 618.]]\n",
            "Validating fine-tuning epoch: 49\n",
            "\n",
            "Validation: Average loss: 0.5361, Accuracy: 162/200 (81.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[119   1  14]\n",
            " [  7  20   1]\n",
            " [ 12   3  23]]\n",
            "Fine-tuning epoch: 50\n",
            "Train Epoch: 50 [256/2000]\n",
            "Train Epoch: 50 [512/2000]\n",
            "Train Epoch: 50 [768/2000]\n",
            "Train Epoch: 50 [1024/2000]\n",
            "Train Epoch: 50 [1280/2000]\n",
            "Train Epoch: 50 [1536/2000]\n",
            "Train Epoch: 50 [1792/2000]\n",
            "Train Epoch: 50 [2000/2000]\n",
            "Train Epoch: 50\t Loss: 0.218908 \t Accuracy: 1900/2000 (95.00%)\n",
            "Confusion matrix for train:\n",
            "[[604.  10.  47.]\n",
            " [  6. 658.   3.]\n",
            " [ 33.   1. 638.]]\n",
            "Validating fine-tuning epoch: 50\n",
            "\n",
            "Validation: Average loss: 0.5521, Accuracy: 158/200 (79.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[111   1  22]\n",
            " [  4  22   2]\n",
            " [ 10   3  25]]\n",
            "Fine-tuning epoch: 51\n",
            "Train Epoch: 51 [256/2000]\n",
            "Train Epoch: 51 [512/2000]\n",
            "Train Epoch: 51 [768/2000]\n",
            "Train Epoch: 51 [1024/2000]\n",
            "Train Epoch: 51 [1280/2000]\n",
            "Train Epoch: 51 [1536/2000]\n",
            "Train Epoch: 51 [1792/2000]\n",
            "Train Epoch: 51 [2000/2000]\n",
            "Train Epoch: 51\t Loss: 0.185722 \t Accuracy: 1922/2000 (96.10%)\n",
            "Confusion matrix for train:\n",
            "[[599.   7.  37.]\n",
            " [  6. 682.   3.]\n",
            " [ 22.   3. 641.]]\n",
            "Validating fine-tuning epoch: 51\n",
            "\n",
            "Validation: Average loss: 0.5652, Accuracy: 154/200 (77.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[107   1  26]\n",
            " [  4  22   2]\n",
            " [ 11   2  25]]\n",
            "Fine-tuning epoch: 52\n",
            "Train Epoch: 52 [256/2000]\n",
            "Train Epoch: 52 [512/2000]\n",
            "Train Epoch: 52 [768/2000]\n",
            "Train Epoch: 52 [1024/2000]\n",
            "Train Epoch: 52 [1280/2000]\n",
            "Train Epoch: 52 [1536/2000]\n",
            "Train Epoch: 52 [1792/2000]\n",
            "Train Epoch: 52 [2000/2000]\n",
            "Train Epoch: 52\t Loss: 0.200745 \t Accuracy: 1893/2000 (94.65%)\n",
            "Confusion matrix for train:\n",
            "[[608.   7.  57.]\n",
            " [  2. 646.   0.]\n",
            " [ 39.   2. 639.]]\n",
            "Validating fine-tuning epoch: 52\n",
            "\n",
            "Validation: Average loss: 0.5100, Accuracy: 164/200 (82.00%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[117   3  14]\n",
            " [  3  23   2]\n",
            " [ 12   2  24]]\n",
            "Fine-tuning epoch: 53\n",
            "Train Epoch: 53 [256/2000]\n",
            "Train Epoch: 53 [512/2000]\n",
            "Train Epoch: 53 [768/2000]\n",
            "Train Epoch: 53 [1024/2000]\n",
            "Train Epoch: 53 [1280/2000]\n",
            "Train Epoch: 53 [1536/2000]\n",
            "Train Epoch: 53 [1792/2000]\n",
            "Train Epoch: 53 [2000/2000]\n",
            "Train Epoch: 53\t Loss: 0.204125 \t Accuracy: 1887/2000 (94.35%)\n",
            "Confusion matrix for train:\n",
            "[[583.  15.  46.]\n",
            " [  6. 680.   2.]\n",
            " [ 43.   1. 624.]]\n",
            "Validating fine-tuning epoch: 53\n",
            "\n",
            "Validation: Average loss: 0.5235, Accuracy: 159/200 (79.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[115   2  17]\n",
            " [  4  22   2]\n",
            " [ 13   3  22]]\n",
            "Fine-tuning epoch: 54\n",
            "Train Epoch: 54 [256/2000]\n",
            "Train Epoch: 54 [512/2000]\n",
            "Train Epoch: 54 [768/2000]\n",
            "Train Epoch: 54 [1024/2000]\n",
            "Train Epoch: 54 [1280/2000]\n",
            "Train Epoch: 54 [1536/2000]\n",
            "Train Epoch: 54 [1792/2000]\n",
            "Train Epoch: 54 [2000/2000]\n",
            "Train Epoch: 54\t Loss: 0.188272 \t Accuracy: 1918/2000 (95.90%)\n",
            "Confusion matrix for train:\n",
            "[[633.  10.  47.]\n",
            " [  1. 663.   2.]\n",
            " [ 20.   2. 622.]]\n",
            "Validating fine-tuning epoch: 54\n",
            "\n",
            "Validation: Average loss: 0.4990, Accuracy: 169/200 (84.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[123   0  11]\n",
            " [  4  22   2]\n",
            " [ 11   3  24]]\n",
            "Fine-tuning epoch: 55\n",
            "Train Epoch: 55 [256/2000]\n",
            "Train Epoch: 55 [512/2000]\n",
            "Train Epoch: 55 [768/2000]\n",
            "Train Epoch: 55 [1024/2000]\n",
            "Train Epoch: 55 [1280/2000]\n",
            "Train Epoch: 55 [1536/2000]\n",
            "Train Epoch: 55 [1792/2000]\n",
            "Train Epoch: 55 [2000/2000]\n",
            "Train Epoch: 55\t Loss: 0.174010 \t Accuracy: 1930/2000 (96.50%)\n",
            "Confusion matrix for train:\n",
            "[[613.   7.  37.]\n",
            " [  1. 675.   2.]\n",
            " [ 22.   1. 642.]]\n",
            "Validating fine-tuning epoch: 55\n",
            "\n",
            "Validation: Average loss: 0.5056, Accuracy: 165/200 (82.50%%)\n",
            "\n",
            "Confusion matrix for Validation:\n",
            "[[123   0  11]\n",
            " [  6  21   1]\n",
            " [ 14   3  21]]\n",
            "\n",
            "Training finished. Best overall validation accuracy: 0.7800\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for param in trainer.model.parameters(): # Access model via trainer\n",
        "    param.requires_grad = True\n",
        "\n",
        "learning_rate_finetune = 1e-5\n",
        "\n",
        "optimizer_finetune = torch.optim.AdamW(trainer.model.parameters(), lr=learning_rate_finetune, weight_decay=weight_decay)\n",
        "\n",
        "# Update the optimizer within the trainer object\n",
        "trainer.optimizer = optimizer_finetune\n",
        "print(f\"Switched to new optimizer with LR: {learning_rate_finetune}\")\n",
        "\n",
        "# 3. Continue training for a few more epochs\n",
        "num_finetune_epochs = 30 # Example: Train for 15 more epochs\n",
        "start_finetune_epoch = best_epoch # Continue epoch numbering for logging\n",
        "\n",
        "trainer.test(valid_loader, \"Validation\", start_finetune_epoch)\n",
        "\n",
        "for epoch in range(start_finetune_epoch, start_finetune_epoch + num_finetune_epochs):\n",
        "    print(f\"Fine-tuning epoch: {epoch}\")\n",
        "    # Use the same training method, it will now use the new optimizer\n",
        "    trainer.train(train_loader, epoch, )\n",
        "\n",
        "    print(f\"Validating fine-tuning epoch: {epoch}\")\n",
        "    val_acc = trainer.test(valid_loader, \"Validation FT\", epoch)\n",
        "\n",
        "    trainer.save_model(f\"models/{name}_{epoch}_ft.pth\", model_name=f\"history/{name}_ft\")\n",
        "    \n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        best_model_file = f\"models/{name}_{epoch}_ft.pth\"\n",
        "        best_epoch = epoch\n",
        "        print(f\"New best accuracy: {best_accuracy}\")\n",
        "\n",
        "print(f\"\\nTraining finished. Best overall validation accuracy: {best_epoch:.4f}\")\n",
        "\n",
        "trainer.plot_model(model_name=\"MobileNet_FT\", path=f\"plots/{name}_ft.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
